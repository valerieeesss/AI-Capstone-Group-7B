# -*- coding: utf-8 -*-
"""AI capstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qO7BojMX_3Zlz1JnwXKZtD-zvsMzde_B
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import pylab as pl
import numpy as np
import scipy.optimize as opt
from sklearn import preprocessing
# %matplotlib inline
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error
from xgboost import XGBRegressor

df_cycle = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CycleData.csv')

# Check for missing values
print("\n\nMissing Values of CycleData:\n")
print(df_cycle.isnull().sum())

# Remove missing values
df_cycle = df_cycle.dropna(subset=['Fuel Used'],axis=0)
df_cycle = df_cycle.dropna(axis=1)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

X = df_cycle.drop('Fuel Used', axis=1)  # Features
y = df_cycle['Fuel Used']  # Target variable
# Take the object var only and change to int type
object_columns = X.select_dtypes(include=['object']).columns
label_encoders = {}
for col in object_columns:
    label_encoders[col] = LabelEncoder()
    X[col] = label_encoders[col].fit_transform(X[col])
#print(X.dtypes)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)
print(y.head())
print(X_train.head())

param_grid = {
    'n_estimators': [100, 200, 300, 400, 500],
    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],
    'max_depth': [3, 4, 5, 6, 7],
    'min_child_weight': [1, 2, 3, 4, 5],
    'gamma': [0, 0.1, 0.2, 0.3, 0.4],
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],
    'reg_alpha': [0, 0.1, 0.5, 1, 2],
    'reg_lambda': [0, 0.1, 0.5, 1, 2]
}

# Initialize XGBRegressor
xgb_regressor = XGBRegressor(objective='reg:squarederror', eval_metric='rmse', random_state=42)

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=xgb_regressor, param_distributions=param_grid, n_iter=100, scoring='neg_mean_squared_error', cv=5, verbose=2, random_state=42, n_jobs=-1)

# Fit the random search to the data
random_search.fit(X_train, y_train)

# Best hyperparameters
print("Best hyperparameters:", random_search.best_params_)

# Initialize and train the XGBoost with optimized hyper parameters
best_params = {
    'subsample': 0.7,
    'reg_lambda': 0.5,
    'reg_alpha': 0.1,
    'n_estimators': 500,
    'min_child_weight': 4,
    'max_depth': 4,
    'learning_rate': 0.3,
    'gamma': 0,
    'colsample_bytree': 0.9
}
# Initialize XGBRegressor with best hyperparameters
xgb_regressor_best = XGBRegressor(objective='reg:squarederror', eval_metric='rmse', random_state=42, **best_params)
xgb_regressor_best.fit(X_train, y_train)
# Predict
y_pred = xgb_regressor_best.predict(X_test)
# Evaluate the model
mse_best = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse_best)

mae_best = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error:", mae_best)

rmse_best = mean_squared_error(y_test, y_pred, squared=False)
print("Root Mean Squared Error:", rmse_best)


import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import probplot

# Calculate residuals
residuals = y_test - y_pred

# Residual Plot
plt.figure(figsize=(10, 6))
plt.scatter(y_pred, residuals, alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--')
plt.title('Residual Plot')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.show()

# Distribution of Errors
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True)
plt.title('Distribution of Errors')
plt.xlabel('Errors')
plt.ylabel('Density')
plt.show()

xgb_regressor = XGBRegressor(objective='reg:squarederror', eval_metric='rmse', random_state=42)
xgb_regressor.fit(X_train, y_train)
# Predict
y_pred = xgb_regressor.predict(X_test)
# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error:", mae)

rmse = mean_squared_error(y_test, y_pred, squared=False)
print("Root Mean Squared Error:", rmse)


import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import probplot

# Calculate residuals
residuals = y_test - y_pred

# Residual Plot
plt.figure(figsize=(10, 6))
plt.scatter(y_pred, residuals, alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--')
plt.title('Residual Plot')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.show()

# Distribution of Errors
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True)
plt.title('Distribution of Errors')
plt.xlabel('Errors')
plt.ylabel('Density')
plt.show()

#improvements
mse_percent= (mse - mse_best) / mse * 100
print("MSE improvement: {:.2f}%".format(mse_percent))

#k-cross validation

from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Define the number of folds for cross-validation
k = 5

# Initialize the cross-validation splitter
kf = KFold(n_splits=k, shuffle=True, random_state=42)

# Initialize your regression model (e.g., LinearRegression)
model = LinearRegression()

# Perform k-fold cross-validation and calculate regression metrics
mse_scores = -cross_val_score(xgb_regressor, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')
r2_scores = cross_val_score(xgb_regressor, X_train, y_train, cv=kf, scoring='r2')

print("Mean Squared Error (MSE) Scores for Each Fold:", mse_scores)
print("R-squared Scores for Each Fold:", r2_scores)
average_mse = mse_scores.mean()
print("Average Mean Squared Error (MSE):", average_mse)
average_r2 = r2_scores.mean()
print("Average R-squared:", average_r2)
#compare
mse_percent2= (average_mse - mse_best) / average_mse * 100
print("MSE improvement: {:.2f}%".format(mse_percent2))

param_grid = {
    'n_estimators': [100, 200, 300, 400, 500],
    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],
    'max_depth': [3, 4, 5, 6, 7],
    'min_child_weight': [1, 2, 3, 4, 5],
    'gamma': [0, 0.1, 0.2, 0.3, 0.4],
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],
    'reg_alpha': [0, 0.1, 0.5, 1, 2],
    'reg_lambda': [0, 0.1, 0.5, 1, 2]
}
#Monte Carlo Simulation Parameters
num_simulations = 5
sample_size = 0.8

results = []
for i in range(num_simulations):
    X_sample, _, y_sample, _ = train_test_split(X, y, train_size=sample_size, random_state=i)
    xgb_regressor = XGBRegressor(objective='reg:squarederror', eval_metric='rmse', random_state=42)
    #Random Search
    random_search = RandomizedSearchCV(estimator=xgb_regressor, param_distributions=param_grid, n_iter=50,
                                       scoring='neg_mean_squared_error', cv=5, verbose=2, random_state=42, n_jobs=-1)
    random_search.fit(X_sample, y_sample)

    #Append Results
    results.append({
        'best_params': random_search.best_params_,
        'best_score': random_search.best_score_
    })
#convert results to dataframe
results_df = pd.DataFrame(results)

print("Mean best score:", results_df['best_score'].mean())
print("Standard deviation of best scores:", results_df['best_score'].std())
print("Best hyperparameters:")
print(results_df['best_params'].mode())

model_best_params = results_df['best_params'].mode().iloc[0]
for key, value in model_best_params.items():
    print(f"{key}: {value}")

import matplotlib.pyplot as plt

# Phase details
phases = [
    'Analysis',
    'Requirements Gathering',
    'System Design',
    'Development',
    'Testing and Integration',
    'Deployment'
]

# Corresponding average variances
average_variances = [
   0.13,
0.08,
1.28,
10.12,
0.14,
0.17
]

# Create a bar graph
plt.figure(figsize=(8, 5.5))
plt.bar(phases, average_variances, color='orange')

# Add titles and labels
plt.title('Average Variance of Each Phase')
plt.xlabel('Phase')
plt.ylabel('Average Variance')

# Rotate the x labels for better readability
plt.xticks(rotation=45, ha='right')

# Show the plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Define the phases and their corresponding budget percentages in the same order
phases = ['Analysis', 'Requirements Gathering', 'System Design', 'Development', 'Testing and Integration', 'Deployment']
budget_percentages = [43, 11, 41, 327, 17, 14]

# Create a pie chart
plt.figure(figsize=(8, 6))
plt.pie(budget_percentages, labels=None, autopct='%1.1f%%', startangle=90, counterclock=False)

# Add labels inside the pie chart
plt.gca().set_prop_cycle(None)  # Reset color cycle
plt.pie(budget_percentages, labels=phases, radius=0.95, startangle=90, counterclock=False)
plt.title('Distribution of Days Across Phases')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Show the pie chart
plt.show()

import matplotlib.pyplot as plt

# Define the phases and their corresponding budget percentages in the same order
phases = ['Analysis', 'Requirements Gathering', 'System Design', 'Development', 'Testing and Integration', 'Deployment']
budget_percentages = [165325, 19100, 64135, 968610, 54200, 66180]


# Create a pie chart
plt.figure(figsize=(8, 6))
plt.pie(budget_percentages, labels=None, autopct='%1.1f%%', startangle=90, counterclock=False)

# Add labels inside the pie chart
plt.gca().set_prop_cycle(None)  # Reset color cycle
plt.pie(budget_percentages, labels=phases, radius=0.95, startangle=90, counterclock=False)
plt.title('Budget Allocation Across Phases (Bottom Up)')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Show the pie chart
plt.show()

import matplotlib.pyplot as plt

# Define the phases and their corresponding budget percentages in the same order
phases = ['Analysis', 'Requirements Gathering', 'System Design', 'Development', 'Testing and Integration', 'Deployment']
budget_percentages = [180000,90000,100000,940000,100000, 90000]


# Create a pie chart
plt.figure(figsize=(8, 6))
plt.pie(budget_percentages, labels=None, autopct='%1.1f%%', startangle=90, counterclock=False)

# Add labels inside the pie chart
plt.gca().set_prop_cycle(None)  # Reset color cycle
plt.pie(budget_percentages, labels=phases, radius=0.95, startangle=90, counterclock=False)
plt.title('Budget Allocation Across Phases (Top Down)')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Show the pie chart
plt.show()