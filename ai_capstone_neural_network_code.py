# -*- coding: utf-8 -*-
"""AI capstone - Neural Network Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BJ0IJQ9UAjI8qjPnP19AzzzkG8t7AhDn
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import pylab as pl
import numpy as np
import scipy.optimize as opt
from sklearn import preprocessing
# %matplotlib inline
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error
from xgboost import XGBRegressor

df_cycle = pd.read_csv('/content/drive/MyDrive/AI Capstone Dataset/CycleData.csv')
# Check for missing values
print("\n\nMissing Values of CycleData:\n")
print(df_cycle.isnull().sum())

# Remove missing values
df_cycle = df_cycle.dropna(subset=['Fuel Used'],axis=0)
df_cycle = df_cycle.dropna(axis=1)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

X = df_cycle.drop('Fuel Used', axis=1)  # Features
y = df_cycle['Fuel Used']  # Target variable
# Take the object var only and change to int type
object_columns = X.select_dtypes(include=['object']).columns
label_encoders = {}
for col in object_columns:
    label_encoders[col] = LabelEncoder()
    X[col] = label_encoders[col].fit_transform(X[col])
#print(X.dtypes)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)
print(y.head())
print(X_train.head())

#Performing Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#Neural Network (NN) Algorithm
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import LabelEncoder
from keras.utils import to_categorical

from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.callbacks import EarlyStopping

#Initialising ANN
ann = tf.keras.models.Sequential()

#Adding first and second hidden layer
ann.add(tf.keras.layers.Dense(units=128,activation="relu"))
ann.add(tf.keras.layers.Dense(units=64,activation="relu"))

#Adding output layer
ann.add(tf.keras.layers.Dense(units=1,activation="linear"))

#Compile mae
ann.compile(optimizer="adam",loss="mean_squared_error",metrics=['mean_absolute_error'])

es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True)

history1 = ann.fit(X_test, y_test, callbacks=[es], epochs=10, batch_size=10, shuffle=True, validation_split=0.2, verbose=1)

#plot
loss = history1.history['loss']
val_loss = history1.history['val_loss']
epochs = range(1,len(loss)+1)
plt.plot(epochs, loss, label='Training Loss')
plt.plot(epochs, val_loss, 'r', label="Validation loss")
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

acc = history1.history['mean_absolute_error']
val_acc = history1.history['val_mean_absolute_error']
plt.plot(epochs, acc, 'y', label='Training MAE')
plt.plot(epochs, val_acc, 'r', label='Validation MAE')
plt.title('Training and validation MAE')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#predict on test data
predictions = ann.predict(X_test[:5])
print("Predicted values are: ", predictions)
print("Real values are: ", y_test[:5])
mae = np.mean(np.abs(predictions - y_test.values))
print("mae: ", mae)

#Optimised code with random search
#Neural Network (NN) Algorithm
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import LabelEncoder
from keras.utils import to_categorical
from sklearn.metrics import mean_squared_error
import random

from keras.models import Sequential
from keras import layers
from keras.layers import Dense, Dropout
from keras.callbacks import EarlyStopping

def create_model(hidden_layers, learning_rate):
    model = keras.Sequential()
    model.add(layers.Dense(units=hidden_layers[0], activation='relu', input_dim=X_train.shape[1]))
    for units in hidden_layers[1:]:
        model.add(layers.Dense(units=units, activation='relu'))
    model.add(layers.Dense(1, activation='linear'))
    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_absolute_error'])
    return model

# Define hyperparameter search space
param_grid = {
    'hidden_layers': [(32,), (64,), (32,32), (64,64)],
    'learning_rate': [0.001, 0.01, 0.1]
}

# Perform random search
best_mae = float('inf')
best_params = None
##best_mse = float('inf')
for _ in range(10):  # Number of random searches
    params = {k: random.choice(v) for k, v in param_grid.items()}
    model = create_model(params['hidden_layers'], params['learning_rate'])
    es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True)
    history = model.fit(X_test, y_test, callbacks=[es], epochs=10, batch_size=10, shuffle=True, validation_split=0.2, verbose=1)

    predictions = model.predict(X_test)

    mse = mean_squared_error(y_test, predictions)
    mae = np.mean(np.abs(predictions - y_test.values))
    if mae < best_mae and mse < best_mse:
        best_mae = mae
        best_params = params

    print()

print("Best MAE:", best_mae)
print("Best parameters:", best_params)

#plot
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

plt.plot(history.history['mean_absolute_error'], label='Training MAE')
plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')
plt.title('Training and validation MAE')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#predict on test data
predictions = model.predict(X_test[:5])
print("Predicted values are: ", predictions)
print("Real values are: ", y_test.values[:5])
print("MAE: ", best_mae)